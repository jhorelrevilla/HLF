{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-Data-ingestion",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing for google colab"
      ],
      "metadata": {
        "id": "w97ksHdDYy_l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT_BU0HYYUDq"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful with the spark version"
      ],
      "metadata": {
        "id": "uaTJJH_GbXkk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "43VNt8cLZMWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "WdMReAvEbJ4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "GdJ9ukB9a04A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting a pyspark session"
      ],
      "metadata": {
        "id": "L8IzcZq1b1uX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "aKNllgCFbx9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful with the spark_root_jar version, look the next link: https://github.com/diana-hep/spark-root/tree/master/jars"
      ],
      "metadata": {
        "id": "QOh7FHR_kL9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark_python = \"/usr/local/bin/python\"\n",
        "spark_root_jar = \"https://github.com/diana-hep/spark-root/blob/master/jars/spark-root_2.11-0.1.18.jar?raw=true\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"1-Data-Ingestion\") \\\n",
        "        .master(\"local\") \\\n",
        "        .config(\"spark.jars\",spark_root_jar) \\\n",
        "        .config(\"spark.jars.packages\",\"org.diana-hep:root4j:0.1.6\") \\\n",
        "        .config(\"spark.pyspark.python\",pyspark_python) \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "d_DZW4-Udx_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "p7ZhxTNTF_hl",
        "outputId": "6dbac68a-ef83-4fce-fa35-5fbd348d958a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd986f74690>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://e06c1b01c421:4041\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>1-Data-Ingestion</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sparkContext.addPyFile(\"utilFunctions.py\")"
      ],
      "metadata": {
        "id": "0jlr8_UIOUF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the data"
      ],
      "metadata": {
        "id": "NPeU51JCg9gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9BjUjFZeA9u",
        "outputId": "1af7a502-d7e9-44b9-c135-eda3bcc18307"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/UCSP/Big-Data/Final-project-data/\"\n",
        "\n",
        "samples = [\"qcd_lepFilter_13TeV\", \"ttbar_lepFilter_13TeV\", \"Wlnu_lepFilter_13TeV\"]\n",
        "\n",
        "requiredColumns = [\n",
        "    \"EFlowTrack\",\n",
        "    \"EFlowNeutralHadron\",\n",
        "    \"EFlowPhoton\",\n",
        "    \"Electron\",\n",
        "    \"MuonTight\",\n",
        "    \"MuonTight_size\",\n",
        "    \"Electron_size\",\n",
        "    \"MissingET\",\n",
        "    \"Jet\"\n",
        "]"
      ],
      "metadata": {
        "id": "7OiAoAi_5pvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "dfList = []\n",
        "\n",
        "for label, sample in enumerate(samples):\n",
        "  print(\"Loading {} sample...\".format(sample))\n",
        "  tmpDF = spark.read \\\n",
        "              .format(\"org.dianahep.sparkroot.experimental\") \\\n",
        "              .load(PATH + sample + \"/*.root\") \\\n",
        "              .select(requiredColumns) \\\n",
        "              .withColumn(\"label\", lit(label))\n",
        "  dfList.append(tmpDF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I88jBNbE6MdU",
        "outputId": "0f9c4ac0-5dbf-4dab-b94d-a9f5eea38ed4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading qcd_lepFilter_13TeV sample...\n",
            "Loading ttbar_lepFilter_13TeV sample...\n",
            "Loading Wlnu_lepFilter_13TeV sample...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge all samples into a single dataframe\n",
        "df = dfList[0]\n",
        "for tmpDF in dfList[1:]:\n",
        "    df = df.union(tmpDF)"
      ],
      "metadata": {
        "id": "834t2e0AI_65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(\"EFlowTrack\").printSchema()"
      ],
      "metadata": {
        "id": "C-bIFJr3JF2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Lorentz Vector and other functions for pTmaps\n",
        "from utilFunctions import *\n",
        "\n",
        "def selection(event, TrkPtMap, NeuPtMap, PhotonPtMap, PTcut=23., ISOcut=0.45):\n",
        "    \"\"\"\n",
        "    This function simulates the trigger selection. \n",
        "    Foreach event the presence of one isolated muon or electron with pT >23 GeV is required\n",
        "    \"\"\"\n",
        "    if event.Electron_size == 0 and event.MuonTight_size == 0: \n",
        "        return False, False, False\n",
        "    \n",
        "    foundMuon = None \n",
        "    foundEle =  None \n",
        "    \n",
        "    l = LorentzVector()\n",
        "    \n",
        "    for ele in event.Electron:\n",
        "        if ele.PT <= PTcut: continue\n",
        "        l.SetPtEtaPhiM(ele.PT, ele.Eta, ele.Phi, 0.)\n",
        "        \n",
        "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
        "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
        "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
        "        if foundEle == None and (pfisoCh+pfisoNeu+pfisoGamma)<ISOcut:\n",
        "            foundEle = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
        "                        0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
        "                        0., 0., 0., 1., 0., float(ele.Charge)]\n",
        "    \n",
        "    for muon in event.MuonTight:\n",
        "        if muon.PT <= PTcut: continue\n",
        "        l.SetPtEtaPhiM(muon.PT, muon.Eta, muon.Phi, 0.)\n",
        "        \n",
        "        pfisoCh = PFIso(l, 0.3, TrkPtMap, True)\n",
        "        pfisoNeu = PFIso(l, 0.3, NeuPtMap, False)\n",
        "        pfisoGamma = PFIso(l, 0.3, PhotonPtMap, False)\n",
        "        if foundMuon == None and (pfisoCh+pfisoNeu+pfisoGamma)<ISOcut:\n",
        "            foundMuon = [l.E(), l.Px(), l.Py(), l.Pz(), l.Pt(), l.Eta(), l.Phi(),\n",
        "                         0., 0., 0., pfisoCh, pfisoGamma, pfisoNeu,\n",
        "                         0., 0., 0., 0., 1., float(muon.Charge)]\n",
        "            \n",
        "    if foundEle != None and foundMuon != None:\n",
        "        if foundEle[5] > foundMuon[5]:\n",
        "            return True, foundEle, foundMuon\n",
        "        else:\n",
        "            return True, foundMuon, foundEle\n",
        "    if foundEle != None: return True, foundEle, foundMuon\n",
        "    if foundMuon != None: return True, foundMuon, foundEle\n",
        "    \n",
        "    return False, None, None"
      ],
      "metadata": {
        "id": "OZKyBV1QNirv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "from pyspark.ml.linalg import Vectors\n",
        "\n",
        "def convert(event):\n",
        "    \"\"\"\n",
        "    This function takes as input an event, applies trigger selection \n",
        "    and create LLF and HLF datasets\n",
        "    \"\"\"\n",
        "    q = LorentzVector()\n",
        "    particles = []\n",
        "    TrkPtMap = ChPtMapp(0.3, event)\n",
        "    NeuPtMap = NeuPtMapp(0.3, event)\n",
        "    PhotonPtMap = PhotonPtMapp(0.3, event)\n",
        "    if TrkPtMap.shape[0] == 0: return Row()\n",
        "    if NeuPtMap.shape[0] == 0: return Row()\n",
        "    if PhotonPtMap.shape[0] == 0: return Row()\n",
        "    \n",
        "    #\n",
        "    # Get leptons\n",
        "    #\n",
        "    selected, lep, otherlep = selection(event, TrkPtMap, NeuPtMap, PhotonPtMap)\n",
        "    if not selected: return Row()\n",
        "    particles.append(lep)\n",
        "    lepMomentum = LorentzVector(lep[1], lep[2], lep[3], lep[0])\n",
        "    \n",
        "    #\n",
        "    # Select Tracks\n",
        "    #\n",
        "    nTrk = 0\n",
        "    for h in event.EFlowTrack:\n",
        "        if nTrk>=450: continue\n",
        "        if h.PT<=0.5: continue\n",
        "        q.SetPtEtaPhiM(h.PT, h.Eta, h.Phi, 0.)\n",
        "        if lepMomentum.DeltaR(q) > 0.0001:\n",
        "            pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
        "            pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
        "            pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
        "            particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
        "                              h.PT, h.Eta, h.Phi, h.X, h.Y, h.Z,\n",
        "                              pfisoCh, pfisoGamma, pfisoNeu,\n",
        "                              1., 0., 0., 0., 0., float(np.sign(h.PID))])\n",
        "            nTrk += 1\n",
        "    \n",
        "    #\n",
        "    # Select Photons\n",
        "    #\n",
        "    nPhoton = 0\n",
        "    for h in event.EFlowPhoton:\n",
        "        if nPhoton >= 150: continue\n",
        "        if h.ET <= 1.: continue\n",
        "        q.SetPtEtaPhiM(h.ET, h.Eta, h.Phi, 0.)\n",
        "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
        "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
        "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
        "        particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
        "                          h.ET, h.Eta, h.Phi, 0., 0., 0.,\n",
        "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
        "                          0., 0., 1., 0., 0., 0.])\n",
        "        nPhoton += 1\n",
        "    \n",
        "    #\n",
        "    # Select Neutrals\n",
        "    #\n",
        "    nNeu = 0\n",
        "    for h in event.EFlowNeutralHadron:\n",
        "        if nNeu >= 200: continue\n",
        "        if h.ET <= 1.: continue\n",
        "        q.SetPtEtaPhiM(h.ET, h.Eta, h.Phi, 0.)\n",
        "        pfisoCh = PFIso(q, 0.3, TrkPtMap, True)\n",
        "        pfisoNeu = PFIso(q, 0.3, NeuPtMap, False)\n",
        "        pfisoGamma = PFIso(q, 0.3, PhotonPtMap, False)\n",
        "        particles.append([q.E(), q.Px(), q.Py(), q.Pz(),\n",
        "                          h.ET, h.Eta, h.Phi, 0., 0., 0.,\n",
        "                          pfisoCh, pfisoGamma, pfisoNeu,\n",
        "                          0., 1., 0., 0., 0., 0.])\n",
        "        nNeu += 1\n",
        "        \n",
        "    for iTrk in range(nTrk, 450):\n",
        "        particles.append([0., 0., 0., 0., 0., 0., 0., 0.,0.,\n",
        "                          0.,0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for iPhoton in range(nPhoton, 150):\n",
        "        particles.append([0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    for iNeu in range(nNeu, 200):\n",
        "        particles.append([0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
        "                          0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])        \n",
        "    #\n",
        "    # High Level Features\n",
        "    #\n",
        "    myMET = event.MissingET[0]\n",
        "    MET = myMET.MET\n",
        "    phiMET = myMET.Phi\n",
        "    MT = 2.*MET*lepMomentum.Pt()*(1-math.cos(lepMomentum.Phi()-phiMET))\n",
        "    HT = 0.\n",
        "    nJets = 0.\n",
        "    nBjets = 0.\n",
        "    for jet in event.Jet:\n",
        "        if jet.PT > 30 and abs(jet.Eta)<2.6:\n",
        "            nJets += 1\n",
        "            HT += jet.PT\n",
        "            if jet.BTag>0: \n",
        "                nBjets += 1\n",
        "    LepPt = lep[4]\n",
        "    LepEta = lep[5]\n",
        "    LepPhi = lep[6]\n",
        "    LepIsoCh = lep[10]\n",
        "    LepIsoGamma = lep[11]\n",
        "    LepIsoNeu = lep[12]\n",
        "    LepCharge = lep[18]\n",
        "    LepIsEle = lep[16]\n",
        "    hlf = Vectors.dense([HT, MET, phiMET, MT, nJets, nBjets, LepPt, LepEta, LepPhi,\n",
        "           LepIsoCh, LepIsoGamma, LepIsoNeu, LepCharge, LepIsEle])     \n",
        "    #\n",
        "    # return the Row of low level features and high level features\n",
        "    #\n",
        "    return Row(lfeatures=particles, hfeatures=hlf, label=event.label)"
      ],
      "metadata": {
        "id": "54b3rDhsNil0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = df.rdd \\\n",
        "            .map(convert) \\\n",
        "            .filter(lambda row: len(row) > 0) \\\n",
        "            .toDF()"
      ],
      "metadata": {
        "id": "OMXp8ewaNije"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# features.printSchema()"
      ],
      "metadata": {
        "id": "7S5WruEHOb9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/UCSP/Big-Data/Final-project-data/data-processed\"\n",
        "num_partitions = 3000 # used in DataFrame coalesce operation to limit number of output files\n",
        "\n",
        "%time features.coalesce(num_partitions).write.partitionBy(\"label\").parquet(dataset_path)"
      ],
      "metadata": {
        "id": "29MLXzLhOcd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of events written to Parquet:\", spark.read.parquet(dataset_path).count())"
      ],
      "metadata": {
        "id": "Bm19_jp8OeA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "a4RAb-Aw--Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "References:\n",
        "- https://colab.research.google.com/github/asifahmed90/pyspark-ML-in-Colab/blob/master/PySpark_Regression_Analysis.ipynb#scrollTo=lh5NCoc8fsSO\n",
        "-"
      ],
      "metadata": {
        "id": "4Y0PF_W1eBXe"
      }
    }
  ]
}