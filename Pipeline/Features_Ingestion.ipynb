{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-Features-Ingestion",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Installing for google colab"
      ],
      "metadata": {
        "id": "_QHyHHHaNAeY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpmBZMWBHjg5"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Be careful with the spark version"
      ],
      "metadata": {
        "id": "O0QMjG9ONFCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://dlcdn.apache.org/spark/spark-3.0.3/spark-3.0.3-bin-hadoop2.7.tgz\n",
        "!tar xf spark-3.0.3-bin-hadoop2.7.tgz"
      ],
      "metadata": {
        "id": "Tqevpc03M3lP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q findspark"
      ],
      "metadata": {
        "id": "VBcGBAWSM6ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.3-bin-hadoop2.7\""
      ],
      "metadata": {
        "id": "vJX63AqNM-Ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Starting a pyspark session"
      ],
      "metadata": {
        "id": "OYJAYV6VNINY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "id": "YnnqHMa5NH1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyspark_python = \"/usr/local/bin/python\"\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "        .appName(\"2-Feature-Preparation\") \\\n",
        "        .master(\"local\") \\\n",
        "        .config(\"spark.pyspark.python\",pyspark_python) \\\n",
        "        .getOrCreate()"
      ],
      "metadata": {
        "id": "4E-3GLj5NYir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "RG7n6MP9OsB3",
        "outputId": "38ed8a1b-6d3d-40a1-95ea-79821d02af11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fb8e58ad5d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://675f024500d4:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>2-Feature-Preparation</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the data"
      ],
      "metadata": {
        "id": "BNZR9r_8QDF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrvIe9vHQBDD",
        "outputId": "0f799051-c3c9-468d-a10b-b897ceeaf96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/content/drive/MyDrive/UCSP/Big-Data/Final-project/data-processed\"\n",
        "\n",
        "data = spark.read \\\n",
        "            .format(\"parquet\") \\\n",
        "            .load(dataset_path)\n",
        "\n",
        "events = data.count()\n",
        "print(\"There are {} events\".format(events))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7C_GEonNP5Ix",
        "outputId": "b3265d12-6f1e-4481-9652-25734b68303c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 18045 events\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['QCD', 'tt', 'W+jets']\n",
        "counts = data.groupBy('label').count().collect()\n",
        "\n",
        "qcd_events = 0\n",
        "tt_events = 0 \n",
        "wjets_events = 0\n",
        "\n",
        "print('There are:')\n",
        "for i in range(3):\n",
        "    print('\\t* {} {} events (frac = {:.3f})'\n",
        "          .format(\n",
        "              counts[i][1],\n",
        "              labels[counts[i].label],\n",
        "              counts[i][1]*1.0/events\n",
        "          ))\n",
        "    if counts[i].label==0:\n",
        "        qcd_events = counts[i][1]\n",
        "    elif counts[i].label==1:\n",
        "        tt_events = counts[i][1] \n",
        "    elif counts[i].label==2:\n",
        "        wjets_events = counts[i][1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26HM-whCiWNU",
        "outputId": "92f68170-36be-441c-fd44-f84791a0b890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are:\n",
            "\t* 4101 tt events (frac = 0.227)\n",
            "\t* 13647 W+jets events (frac = 0.756)\n",
            "\t* 297 QCD events (frac = 0.016)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature preparation"
      ],
      "metadata": {
        "id": "Z_QRaqYXieWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKBvQg8Dif93",
        "outputId": "8deeda2e-5f1a-4109-ce63-16ce05c31e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- hfeatures: vector (nullable = true)\n",
            " |-- lfeatures: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: double (containsNull = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "\n",
        "vector_dense_udf = udf(lambda r : Vectors.dense(r),VectorUDT())\n",
        "data = data.withColumn('hfeatures_dense',vector_dense_udf('hfeatures'))"
      ],
      "metadata": {
        "id": "_eJ7HL9oixRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "\n",
        "## One-Hot-Encode\n",
        "# Use OneHotEncoderEstimator for Spark 2.x and OneHotEncoder for Spark 3.x\n",
        "encoder = OneHotEncoder(inputCols=[\"label\"],\n",
        "                        outputCols=[\"encoded_label\"],\n",
        "                        dropLast=False)\n",
        "\n",
        "## Scale feature vector\n",
        "scaler = MinMaxScaler(inputCol=\"hfeatures_dense\",\n",
        "                      outputCol=\"HLF_input\")\n",
        "\n",
        "pipeline = Pipeline(stages=[encoder, scaler])\n",
        "\n",
        "%time fitted_pipeline = pipeline.fit(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hq9enI5Bi345",
        "outputId": "ee4a4f28-9372-4bb7-f0db-df1923d8764f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 68.4 ms, sys: 10.4 ms, total: 78.8 ms\n",
            "Wall time: 11.3 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the pipeline to data\n",
        "data = fitted_pipeline.transform(data)"
      ],
      "metadata": {
        "id": "FmdVKBSdi-oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1F33zB1FjEqb",
        "outputId": "4bb8736f-08e7-40f1-8acf-5d022ab6a259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- hfeatures: vector (nullable = true)\n",
            " |-- lfeatures: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: double (containsNull = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- hfeatures_dense: vector (nullable = true)\n",
            " |-- encoded_label: vector (nullable = true)\n",
            " |-- HLF_input: vector (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class lepAngularCoordinates():\n",
        "    \"\"\"\n",
        "    This class is used to store the lepton and compute DeltaR \n",
        "    from the other particles\n",
        "    \"\"\"\n",
        "    def __init__(self, eta, phi):\n",
        "        self.Eta = eta\n",
        "        self.Phi = phi\n",
        "    \n",
        "    def DeltaR(self, eta, phi):\n",
        "        deta = self.Eta - eta\n",
        "        \n",
        "        dphi = self.Phi - phi       \n",
        "        pi = math.pi\n",
        "        while dphi >  pi: dphi -= 2*pi\n",
        "        while dphi < -pi: dphi += 2*pi\n",
        "            \n",
        "        return math.sqrt(deta*deta + dphi*dphi)"
      ],
      "metadata": {
        "id": "NkNjQuc3jXS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import ArrayType, DoubleType\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "@udf(returnType=ArrayType(ArrayType(DoubleType())))\n",
        "def transform(particles):\n",
        "    ## The isolated lepton is the first partiche in the list\n",
        "    ISOlep = lepAngularCoordinates(particles[0][5], particles[0][6])\n",
        "    \n",
        "    ## Sort the particles based on the distance from the isolated lepton\n",
        "    particles.sort(key = lambda part: ISOlep.DeltaR(part[5], part[6]),\n",
        "                   reverse=True)\n",
        "    \n",
        "    ## Standardize\n",
        "    particles = StandardScaler().fit_transform(particles).tolist()\n",
        "    \n",
        "    return particles"
      ],
      "metadata": {
        "id": "OqO8XkIrjeVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.withColumn('GRU_input', transform('lfeatures'))"
      ],
      "metadata": {
        "id": "QBqFoOy8jgN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-7YvSMcjh4_",
        "outputId": "eb8d616c-aa6d-49d6-826e-3eeec7416f54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- hfeatures: vector (nullable = true)\n",
            " |-- lfeatures: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: double (containsNull = true)\n",
            " |-- label: integer (nullable = true)\n",
            " |-- hfeatures_dense: vector (nullable = true)\n",
            " |-- encoded_label: vector (nullable = true)\n",
            " |-- HLF_input: vector (nullable = true)\n",
            " |-- GRU_input: array (nullable = true)\n",
            " |    |-- element: array (containsNull = true)\n",
            " |    |    |-- element: double (containsNull = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Undersampled the dataset"
      ],
      "metadata": {
        "id": "JAVAGEL8jm7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qcd = data.filter('label=0')\n",
        "tt = data.filter('label=1')\n",
        "wjets = data.filter('label=2')"
      ],
      "metadata": {
        "id": "0bB165u3jmVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the undersampled dataframes\n",
        "# False means to sample without repetition\n",
        "tt = tt.sample(False, qcd_events*1.0/tt_events) \n",
        "wjets = wjets.sample(False, qcd_events*1.0/wjets_events)\n",
        "\n",
        "dataUndersampled = qcd.union(tt).union(wjets)"
      ],
      "metadata": {
        "id": "q0d4NCCyjqyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataUndersampled.groupBy('label').count().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ogoLmcXjuZI",
        "outputId": "44776312-3fe0-4dd4-d997-5df5a34bea8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----+\n",
            "|label|count|\n",
            "+-----+-----+\n",
            "|    1|  292|\n",
            "|    2|  328|\n",
            "|    0|  297|\n",
            "+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shuffle the dataset"
      ],
      "metadata": {
        "id": "Ckr2nL1wjxKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import rand \n",
        "trainUndersampled, testUndersampled = dataUndersampled.randomSplit([0.8, 0.2], seed=42)\n",
        "trainUndersampled = trainUndersampled.orderBy(rand(seed=42))"
      ],
      "metadata": {
        "id": "ROY1O_u-jwY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataset as Apache Parquet files"
      ],
      "metadata": {
        "id": "RNoOeDLvj8cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"/content/drive/MyDrive/UCSP/Big-Data/Final-project/data-for-training/\"\n",
        "\n",
        "numTestPartitions = 1\n",
        "\n",
        "%time testUndersampled.coalesce(numTestPartitions).write.parquet(PATH + 'testUndersampled.parquet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Rteeo8j70L",
        "outputId": "fe57ac12-45e4-47d9-9561-114f64207082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.93 s, sys: 338 ms, total: 3.27 s\n",
            "Wall time: 10min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numTrainPartitions = 1\n",
        "\n",
        "%time trainUndersampled.coalesce(numTrainPartitions).write.parquet(PATH + 'trainUndersampled.parquet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZV3BSajkWTk",
        "outputId": "d5e4a3da-6697-4545-da6a-ec0df2680d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 3.67 s, sys: 464 ms, total: 4.14 s\n",
            "Wall time: 13min 28s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.stop()"
      ],
      "metadata": {
        "id": "nRfpj5NTOtcN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}